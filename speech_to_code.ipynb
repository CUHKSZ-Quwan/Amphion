{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tts.gpt_tts.gpt_tts import GPTTTS\n",
    "from models.tts.gpt_tts.g2p_old_en import process, PHPONE2ID\n",
    "from g2p_en import G2p\n",
    "from models.codec.codec_latent.codec_latent import LatentCodecEncoder, LatentCodecDecoderWithTimbre\n",
    "from models.codec.amphion_codec.codec import CodecEncoder, CodecDecoder\n",
    "from utils.util import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_type': 'LaCoTTS', 'dataset': ['Your Dataset Name'], 'preprocess': {'hop_size': 200, 'sample_rate': 16000, 'processed_dir': '', 'valid_file': 'valid.json', 'train_file': 'train.json'}, 'model': {'latent_codec': {'encoder': {'d_mel': 128, 'd_model': 96, 'num_blocks': 4, 'out_channels': 256, 'use_tanh': False}, 'decoder': {'in_channels': 256, 'num_quantizers': 1, 'codebook_size': 8192, 'codebook_dim': 8, 'quantizer_type': 'fvq', 'use_l2_normlize': True, 'vocos_dim': 512, 'vocos_intermediate_dim': 4096, 'vocos_num_layers': 16, 'ln_before_vq': True, 'use_pe': False}, 'pretrained_ckpt': '...'}, 'wav_codec': {'encoder': {'d_model': 96, 'out_channels': 128, 'up_ratios': [2, 4, 5, 5], 'use_tanh': False}, 'decoder': {'in_channels': 128, 'upsample_initial_channel': 1536, 'num_quantizers': 8, 'codebook_size': 1024, 'codebook_dim': 128, 'quantizer_type': 'fvq', 'quantizer_dropout': 0.5, 'use_l2_normlize': True, 'use_vocos': True, 'vocos_dim': 512, 'vocos_intermediate_dim': 4096, 'vocos_num_layers': 24}, 'pretrained_ckpt': '...'}, 'gpt_tts': {'phone_vocab_size': 256, 'target_vocab_size': 8192, 'hidden_size': 1024, 'intermediate_siz': 4096, 'num_hidden_layer': 12, 'num_attention_head': 16, 'pad_token_id': 8448, 'bos_target_id': 8449, 'eos_target_id': 8450, 'bos_phone_id': 8451, 'eos_phone_id': 8452, 'max_position_embeddings': 2048}}, 'log_dir': 'Your log dir', 'train': {'max_epoch': 0, 'use_dynamic_batchsize': True, 'max_tokens': 1600000, 'max_sentences': 16, 'lr_warmup_steps': 32000, 'lr_scheduler': 'inverse_sqrt', 'num_train_steps': 800000, 'adam': {'lr': 0.0001}, 'ddp': False, 'random_seed': 114, 'batch_size': 16, 'epochs': 5000, 'max_steps': 1000000, 'total_training_steps': 800000, 'save_summary_steps': 500, 'save_checkpoints_steps': 2000, 'valid_interval': 2000, 'keep_checkpoint_max': 100, 'gradient_accumulation_step': 1, 'tracker': ['tensorboard'], 'save_checkpoint_stride': [1], 'keep_last': [1000], 'run_eval': [True], 'dataloader': {'num_worker': 16, 'pin_memory': True}}}\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"egs/tts/LaCoTTS/exp_config_base.json\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_codec_enc = CodecEncoder(\n",
    "    cfg=cfg.model.wav_codec.encoder\n",
    ")\n",
    "wav_codec_dec = CodecDecoder(\n",
    "    cfg=cfg.model.wav_codec.decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_codec_enc = LatentCodecEncoder(\n",
    "    cfg=cfg.model.latent_codec.encoder\n",
    ")\n",
    "latent_codec_dec = LatentCodecDecoderWithTimbre(\n",
    "    cfg=cfg.model.latent_codec.decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentCodecDecoderWithTimbre(\n",
       "  (quantizer): ResidualVQ(\n",
       "    (quantizers): ModuleList(\n",
       "      (0): FactorizedVectorQuantize(\n",
       "        (in_project): Conv1d(256, 8, kernel_size=(1,), stride=(1,))\n",
       "        (out_project): Conv1d(8, 256, kernel_size=(1,), stride=(1,))\n",
       "        (codebook): Embedding(8192, 8)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Sequential(\n",
       "    (0): VocosBackbone(\n",
       "      (embed): Conv1d(256, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (convnext): ModuleList(\n",
       "        (0-15): 16 x ConvNeXtBlock(\n",
       "          (dwconv): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), groups=512)\n",
       "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (pwconv1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=4096, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (timbre_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ffn): TransformerFFNLayer(\n",
       "          (ffn_1): Conv1d(256, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          (ffn_2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (last_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (timbre_linear): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (timbre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "  (enc_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_codec_enc.load_state_dict(torch.load(\"ckpts/wav_codec/wav_codec_enc.bin\"))\n",
    "wav_codec_dec.load_state_dict(torch.load(\"ckpts/wav_codec/wav_codec_dec.bin\"))\n",
    "latent_codec_enc.load_state_dict(torch.load(\"ckpts/latent_codec/latent_codec_enc.bin\"))\n",
    "latent_codec_dec.load_state_dict(torch.load(\"ckpts/latent_codec/latent_codec_dec.bin\"))\n",
    "\n",
    "wav_codec_enc.eval()\n",
    "wav_codec_dec.eval()\n",
    "latent_codec_enc.eval()\n",
    "latent_codec_dec.eval()\n",
    "\n",
    "wav_codec_enc.cuda()\n",
    "wav_codec_dec.cuda()\n",
    "latent_codec_enc.cuda()\n",
    "latent_codec_dec.cuda()\n",
    "\n",
    "# requires_grad false\n",
    "wav_codec_enc.requires_grad_(False)\n",
    "wav_codec_dec.requires_grad_(False)\n",
    "latent_codec_enc.requires_grad_(False)\n",
    "latent_codec_dec.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_wav = librosa.load(\"examples/ref/1.wav\", sr=16000)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 272])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speech tokenize\n",
    "prompt_wav = torch.FloatTensor(source_wav).cuda().unsqueeze(0)\n",
    "# wav to latent\n",
    "vq_emb = wav_codec_enc(prompt_wav.unsqueeze(1))\n",
    "vq_emb = latent_codec_enc(vq_emb)\n",
    "# latent to token\n",
    "(\n",
    "    _,\n",
    "    vq_indices,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = latent_codec_dec(vq_emb, vq=True, eval_vq=False, return_spk_embs=False)\n",
    "prompt_id = vq_indices[0,:,:]\n",
    "prompt_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n",
      "108844\n"
     ]
    }
   ],
   "source": [
    "code = prompt_id[0].cpu().numpy().tolist()\n",
    "code = [int(c) for c in code]\n",
    "code = np.array(code, dtype=np.int16)\n",
    "# save code as small as possible\n",
    "code.tofile(\"examples/ref/1.code\")\n",
    "# compute the size of the code\n",
    "print(os.path.getsize(\"examples/ref/1.code\"))\n",
    "print(os.path.getsize(\"examples/ref/1.wav\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
